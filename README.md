```markdown
# LSTM-Based Bitcoin Price Forecasting with Boids Features

## Introduction

This project explores the use of Long Short-Term Memory (LSTM) networks for predicting the hourly closing price of the Bitcoin (BTC/USDT pair), using historical data from the Binance exchange. A key focus of this work was investigating whether incorporating novel features derived from the Boids algorithm—a simulation of collective flocking behavior—could enhance prediction accuracy compared to using standard price-based features alone.

The project involved several stages:
1.  Extensive dataset generation (642 datasets) by varying Boids simulation parameters and historical data lengths (1k to 20k hourly records). The 20000-record set remained incomplete (126 out of 192 combinations missing) due to time constraints.
2.  Initial experiments to identify promising Boids parameter configurations using fixed LSTM settings.
3.  Evaluation of LSTM models trained with and without Boids features on a selected dataset (\texttt{LIMIT=10000}, Boids Config: \texttt{400, 100, 10, 150}) using refined hyperparameters (\texttt{LEARNING_RATE=5e-3}, \texttt{EPOCHS=3000}).

The best result obtained in this study was for the model including Boids features, achieving a Mean Absolute Error (MAE) of \$527.47 on the test set. A direct comparison with the model without Boids features under identical hyperparameters was planned but not completed within the project's timeframe.

This project was developed by Oleg Shchendrigin and Dmitrii Ryazanov at Innopolis University. (Initial team member Georgii Iakovlev did not contribute significantly to the final project).

## Project Structure

The repository contains the core scripts, experiment utilities, and results directories. A possible organization reflecting the project components:

```
REPO_ROOT/
│
├── BTC_Prediction/           # Main application/final model scripts (Suggested)
│   ├── .env                  # Configuration file (Template below)
│   ├── boids.py              # Boids simulation logic
│   ├── load_data.py          # Binance data loading and basic preprocessing
│   ├── model.py              # Main script: Train/eval LSTM WITH Boids features
│   └── noboids.py            # Main script: Train/eval LSTM WITHOUT Boids features
│
├── datasets/                 # Directory for storing generated CSV/JSON datasets
│   └── ... (e.g., 10000_400_100_10_150.csv / .json) ...
│
├── experiments/              # Scripts and results from the initial parameter sweep
│   ├── datamore.py           # Script used to generate the 642 datasets
│   ├── top_results_generate.py # Script to parse experiment summaries
│   └── summaries/            # Directory containing CSV/TXT summaries from Phase 1
│       └── ... (experiment_summary_*.csv, top_results_summary.txt) ...
│
├── experiment_results_final/ # Directory for storing results of final runs
│   └── 10000_400_100_10_150_with_boids/
│       ├── best_model_with_boids.pth
│       ├── best_model_with_boids_losses_final.png
│       ├── evaluation_plot_with_boids_final.png
│       └── summary_with_boids.json
│   └── 10000_400_100_10_150_no_boids/ # Results for no_boids run needed here
│       └── ...
│
├── requirements.txt          # Python dependencies (Example below)
└── README.md                 # This file
```

## Features Used

* **Base Features:**
    * `close`: Closing price of the asset.
    * `ma_close`: Simple Moving Average of the closing price.
* **Boids Features (Used in `model.py`):** Generated by `boids.py`:
    * `boids_mean_x`, `boids_mean_y`: Mean position of the flock.
    * `boids_mean_vx`, `boids_mean_vy`: Mean velocity of the flock.
    * `boids_std_x`, `boids_std_y`: Standard deviation of flock positions.
    * `boids_std_vx`, `boids_std_vy`: Standard deviation of flock velocities.

## Methodology Overview

1.  **Data Loading/Generation:** Fetch hourly BTC/USDT data (Binance API via `load_data.py`). If pre-processed data (CSV + JSON with scaling info) for specific parameters is not found in `datasets/`, generate it: calculate MA and `future_close`, run Boids simulation (`boids.py`), combine features, handle NaNs, scale features/target (`MinMaxScaler`), save files (`load_or_generate_data` function).
2.  **Sequence Creation:** Transform scaled time series into sequences for LSTM (`create_sequences`).
3.  **Data Splitting:** Split chronologically into train/test sets (`train_test_split`).
4.  **Model Training:** Train LSTM (`LSTMModel`) using Adam optimizer, MSE loss. Save best model checkpoint based on validation loss (`train_model`). Read hyperparameters from `.env`.
5.  **Evaluation:** Load best model checkpoint. Make predictions on test set, denormalize, calculate MAE/RMSE (`evaluate_model`).
6.  **Comparison:** The framework allows comparison by running `model.py` (with Boids) and `noboids.py` (without Boids) using identical hyperparameters from `.env`.

## Setup and Installation

**1. Prerequisites:**
* Python 3.x
* pip

**2. Clone Repository:**
```bash
git clone [https://github.com/Quartz-Admirer/NIC_final.git](https://github.com/Quartz-Admirer/NIC_final.git)
cd NIC_final
```

**3. Create and Populate `requirements.txt`:**
   Create a file named `requirements.txt` with the following content (adjust versions if needed):
   ```txt
   torch
   pandas
   numpy
   scikit-learn
   matplotlib
   python-dotenv
   requests
   # Add any other specific libraries used
   ```

**4. Install Dependencies:**
```bash
pip install -r requirements.txt
```

**5. Create `.env` File:**
   Create a file named `.env` in the root directory. Use the following template, ensuring parameters match the desired dataset and model configuration:

   ```dotenv
   # .env file
   # Data Parameters (MUST match the dataset you want to load/generate)
   BASE_DATA_PATH=datasets
   DATA_LIMIT=10000
   NUM_BOIDS=400
   DIMENSION=100
   MAX_SPEED=10
   PERCEPTION_RADIUS=150
   TARGET_COL=future_close
   MA_WINDOW=50

   # Model & Training Parameters (Use these for the final comparison)
   SEQ_LENGTH=20
   TRAIN_RATIO=0.8
   HIDDEN_DIM=64
   NUM_LAYERS=2
   LEARNING_RATE=5e-3 # Final LR used for best results
   EPOCHS=3000      # Final Epochs used for best results
   DEVICE=auto # Options: 'auto', 'cuda', 'cpu'

   # Output & Logging Parameters
   RESULTS_BASE_DIR=experiment_results_final
   PLOT_LOSSES=True
   PLOT_EVALUATION=True
   ```

## Usage

Ensure the `.env` file is configured correctly.

**1. Run Training & Evaluation with Boids:**
```bash
python model.py
```
   * Loads/generates data matching `.env`. Trains using all features. Saves results (model, plots, summary) to a subdirectory in `RESULTS_BASE_DIR` named like `..._with_boids/`.

**2. Run Training & Evaluation without Boids:**
```bash
python noboids.py
```
   * Loads/generates the *same* base data file. Trains using only `close`, `ma_close`. Saves results to a subdirectory named like `..._no_boids/`.

**3. Generate Multiple Datasets (Optional):**
   * Modify and run `experiments/datamore.py` to populate the `datasets/` folder.

**4. Parse Experiment Results (Optional):**
   * Use `experiments/top_results_generate.py` to analyze summary files generated during Phase 1 experiments.

## Datasets

Pre-generated datasets from the initial parameter sweep (642 combinations) are available for download:

* **Google Drive Link:** [https://drive.google.com/drive/folders/1Xgijksr7bPRmxFRowSb3J_VO0HQSZLT3?usp=sharing](https://drive.google.com/drive/folders/1Xgijksr7bPRmxFRowSb3J_VO0HQSZLT3?usp=sharing)

These datasets vary in `DATA_LIMIT`, `NUM_BOIDS`, `DIMENSION`, `MAX_SPEED`, and `PERCEPTION_RADIUS`.

## Results Summary

The final evaluation focused on the `DATA_LIMIT=10000` dataset with Boids parameters (`num_boids=400`, `dimension=100`, `max_speed=10`, `perception_radius=150`) and optimized LSTM hyperparameters (`HIDDEN_DIM=64`, `NUM_LAYERS=2`, `LEARNING_RATE=5e-3`, `EPOCHS=3000`).

* **Model with Boids Features:**
    * Test MAE: **527.47**
    * Test RMSE: **809.32**
* **Model without Boids Features:**
    * Test MAE: *(Requires running `noboids.py` with identical final hyperparameters)*
    * Test RMSE: *(Requires running `noboids.py` with identical final hyperparameters)*

The best results obtained in this study were achieved by the model including Boids features. However, a direct comparison necessitates completing the corresponding run for the model without Boids using the final, tuned hyperparameters. The achieved MAE of ~$527 represents the current benchmark for this specific configuration.

## Limitations and Future Work

* The comparative analysis between models with/without Boids under final tuned hyperparameters is incomplete.
* Hyperparameter tuning was performed manually; automated optimization could yield further improvements.
* The Boids simulation rules were simple and independent of market state.
* Evaluation relied primarily on MAE/RMSE. Directional accuracy and backtesting metrics were not explored.
* Comparison against strong non-deep-learning baselines was not performed.

Future work should focus on completing the comparative analysis, performing joint hyperparameter optimization, exploring advanced architectures (e.g., Attention, Transformers), integrating market feedback into Boids, adding diverse features, and evaluating with more application-oriented metrics.

## Authors

* Oleg Shchendrigin
* Dmitrii Ryazanov

*(Initial team member Georgii Iakovlev did not contribute significantly to the final project).*

## Code Links

* **Main Project Repository:** [https://github.com/Quartz-Admirer/NIC_final/tree/main](https://github.com/Quartz-Admirer/NIC_final/tree/main)
* **Datasets on Google Drive:** [https://drive.google.com/drive/folders/1Xgijksr7bPRmxFRowSb3J_VO0HQSZLT3?usp=sharing](https://drive.google.com/drive/folders/1Xgijksr7bPRmxFRowSb3J_VO0HQSZLT3?usp=sharing)

```
